services:
  backend: &backend
    build:
      context: .
      dockerfile: apps/backend/Dockerfile
      target: development
      args:
        PORT: $PORT
    ports:
      - "3000:$PORT"
    env_file:
      - .env
    volumes:
      - ./apps:/app/apps
      - ./apps/backend/data:/data
      - /app/node_modules
    depends_on:
      ollama:
        condition: service_healthy
      s3:
        condition: service_healthy
      init-s3:
        condition: service_completed_successfully
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-sf",
          "-H",
          "Content-Type: application/json",
          "--data",
          '{"query":"{ health }"}',
          "http://localhost:$PORT/graphql",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  frontend:
    build:
      context: .
      dockerfile: apps/frontend/Dockerfile
      target: development
      args:
        FRONTEND_PORT: $FRONTEND_PORT
    ports:
      - "4200:$FRONTEND_PORT"
    volumes:
      - ./apps/frontend:/app/apps/frontend
      - /app/node_modules
    depends_on:
      backend:
        condition: service_healthy
      s3:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:$FRONTEND_PORT"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  backend-e2e:
    <<: *backend
    profiles:
      - backend-e2e
    build:
      context: .
      dockerfile: apps/backend/Dockerfile
      target: production
      args:
        PORT: $PORT

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama-data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        ollama serve &
        pid=$$!
        echo "Waiting for Ollama to start..."
        sleep 5
        echo "Pulling llama3.2:1b model..."
        ollama pull llama3.2:1b
        echo "Model pulled successfully!"
        wait $$pid

  # Takes up to 1 minute to start!
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "8080:8080"
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_BASE_URL=$OLLAMA_BASE_URL
      - WEBUI_AUTH=False
    restart: on-failure
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  s3:
    image: minio/minio:RELEASE.2025-02-07T23-21-09Z-cpuv1
    ports:
      - "9000:9000"
      - "9001:9001" # Dashboard
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server --console-address ":9001" /data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 5s
      timeout: 5s
      retries: 3
    volumes:
      - minio-data:/data

  init-s3:
    image: minio/mc
    depends_on:
      s3:
        condition: service_healthy
    volumes:
      - ./apps/backend/data/example-novel:/source-data
      - ./local-setup/bucket-policy.json:/bucket-policy.json:ro
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set my_minio http://s3:9000 minioadmin minioadmin;
      /usr/bin/mc admin info my_minio;
      /usr/bin/mc mb my_minio/smart-novel || true;
      /usr/bin/mc cp /source-data/71a3fcd8-937d-4e3d-94de-a643ff515f82.png my_minio/smart-novel/covers/71a3fcd8-937d-4e3d-94de-a643ff515f82.png;
      /usr/bin/mc anonymous set-json /bucket-policy.json my_minio/smart-novel;
      echo 'MinIO initialization complete! Public read access granted to covers/ directory only.';"

volumes:
  ollama-data:
    driver: local
  minio-data:
    driver: local
